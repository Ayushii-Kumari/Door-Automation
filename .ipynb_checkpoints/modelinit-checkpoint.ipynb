{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f85c163-8fc1-4504-9379-901c2794f7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (4.11.0.86)\n",
      "Requirement already satisfied: face_recognition in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: torch in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (2.7.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (0.22.1)\n",
      "Requirement already satisfied: face-recognition-models>=0.3.0 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from face_recognition->-r requirements.txt (line 2)) (0.3.0)\n",
      "Requirement already satisfied: Click>=6.0 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from face_recognition->-r requirements.txt (line 2)) (8.1.7)\n",
      "Requirement already satisfied: dlib>=19.7 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from face_recognition->-r requirements.txt (line 2)) (19.24.99)\n",
      "Requirement already satisfied: Pillow in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from face_recognition->-r requirements.txt (line 2)) (10.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 4)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 4)) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 4)) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 4)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 4)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 4)) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 4)) (69.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kiit0001\\appdata\\roaming\\python\\python312\\site-packages (from Click>=6.0->face_recognition->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 4)) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r \"requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e44640b-020e-4466-93e1-1430dd4d4bda",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mSilentFaceAntiSpoofing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m test\n\u001b[0;32m      9\u001b[0m ENCODINGS_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoor auto/encodings.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m LOG_FILE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoor auto/SilentFaceAntiSpoofing/log.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\door auto\\SilentFaceAntiSpoofing\\test.py:15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manti_spoof_predict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AntiSpoofPredict\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerate_patches\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CropImage\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutility\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_model_name\n",
      "File \u001b[1;32m~\\door auto\\SilentFaceAntiSpoofing\\src\\anti_spoof_predict.py:16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mMiniFASNet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MiniFASNetV1, MiniFASNetV2,MiniFASNetV1SE,MiniFASNetV2SE\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_io\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transform \u001b[38;5;28;01mas\u001b[39;00m trans\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutility\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_kernel, parse_model_name\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import os, time, pickle\n",
    "import cv2, face_recognition, numpy as np\n",
    "import torch, torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "import time\n",
    "from SilentFaceAntiSpoofing.test import test\n",
    "\n",
    "ENCODINGS_PATH = r'door auto/encodings.pickle'\n",
    "LOG_FILE = r'door auto/SilentFaceAntiSpoofing/log.txt'\n",
    "MODEL_DIR = r'door auto/SilentFaceAntiSpoofing/resources/anti_spoof_models'\n",
    "DEVICE_ID = 0\n",
    "\n",
    "def save_encoding(encodings, save_path):\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(encodings, f)\n",
    "\n",
    "def load_encoding(save_path):\n",
    "    with open(save_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "if os.path.exists('encodings.pickle'):\n",
    "    encodeDict = load_encoding('encodings.pickle')\n",
    "\n",
    "def recognize_face(frame):\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    enc = face_recognition.face_encodings(rgb)\n",
    "    if not enc:\n",
    "        return None\n",
    "    face_enc = enc[0]\n",
    "    names = list(encodeDict.keys())\n",
    "    known_encodings = list(encodeDict.values())\n",
    "    distances = face_recognition.face_distance(known_encodings, face_enc)\n",
    "    if len(distances) == 0:\n",
    "        return None\n",
    "    best_idx = np.argmin(distances)\n",
    "    if distances[best_idx] < 0.6:\n",
    "        return names[best_idx]\n",
    "    return \"Unknown Person\"\n",
    "\n",
    "def do_action(action, args):\n",
    "    frame = cv2.imread(args.image)\n",
    "    if frame is None:\n",
    "        print(f\"Error: Could not read image {args.image}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        live_label = test(image=frame, model_dir=MODEL_DIR, device_id=DEVICE_ID)\n",
    "    except Exception as e:\n",
    "        print(\"Failed during anti-spoofing inference. Check model path and dependencies.\")\n",
    "        print(\"Details:\", e)\n",
    "        return\n",
    "\n",
    "    if live_label != 1:\n",
    "        print(\"âš  Spoofing attempt detected! Authentication failed.\")\n",
    "        return\n",
    "\n",
    "    # Recognition\n",
    "    name = recognize_face(frame)\n",
    "\n",
    "    if action == 'register':\n",
    "        if name not in ['unknown_person', 'no_persons_found']:\n",
    "            print(f\"User '{name}' already registered.\")\n",
    "            return\n",
    "        if not args.name:\n",
    "            print(\"--name is required for register action.\")\n",
    "            return\n",
    "        encs = face_recognition.face_encodings(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))[0]\n",
    "        encodeDict[args.name] = encs\n",
    "        with open(os.path.join(DB_PATH, f\"{args.name}.pickle\"), 'wb') as f:\n",
    "            pickle.dump(encs, f)\n",
    "        save_encoding()\n",
    "        print(f\"âœ… Registered new user: {args.name}\")\n",
    "        return\n",
    "\n",
    "    if name in ['unknown_person', 'no_persons_found']:\n",
    "        print(\"Unknown user. Please register new user first.\")\n",
    "        return\n",
    "\n",
    "    # Log the event\n",
    "    timestamp = datetime.datetime.now().isoformat()\n",
    "    mode = 'in' if action == 'login' else 'out'\n",
    "    with open(LOG_FILE, 'a') as f:\n",
    "        f.write(f\"{name},{timestamp},{mode}\\n\")\n",
    "\n",
    "    if action == 'login':\n",
    "        print(f\"âœ… Welcome, {name}!\")\n",
    "    else:\n",
    "        print(f\"ðŸ‘‹ Goodbye, {name}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bce760ee-f344-46f8-ae58-e9c3fff09681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Initializing models...\n",
      "Models initialized.\n",
      "Loading known encodings...\n",
      "Loaded 111 known encodings.\n",
      "--- REGISTERING A NEW USER ---\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 214\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- REGISTERING A NEW USER ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    213\u001b[0m register_args \u001b[38;5;241m=\u001b[39m MockArgs(image_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_face.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJohn Doe\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 214\u001b[0m \u001b[43mdo_action\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mregister\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregister_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[37], line 185\u001b[0m, in \u001b[0;36mdo_action\u001b[1;34m(action, args)\u001b[0m\n\u001b[0;32m    183\u001b[0m frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(args\u001b[38;5;241m.\u001b[39mimage)\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Could not read image \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m); \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m live_label \u001b[38;5;241m=\u001b[39m \u001b[43mis_real_face\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m live_label \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâš  Spoofing attempt detected! Authentication failed.\u001b[39m\u001b[38;5;124m\"\u001b[39m); \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Liveness check passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[37], line 156\u001b[0m, in \u001b[0;36mis_real_face\u001b[1;34m(image_frame)\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: Could not parse model name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Skipping. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m); \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     param \u001b[38;5;241m=\u001b[39m { \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg_img\u001b[39m\u001b[38;5;124m\"\u001b[39m: image_frame, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m\"\u001b[39m: image_bbox, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m\"\u001b[39m: scale, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_w\u001b[39m\u001b[38;5;124m\"\u001b[39m: w, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_h\u001b[39m\u001b[38;5;124m\"\u001b[39m: h }\n\u001b[1;32m--> 156\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mimage_cropper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     prediction \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m anti_spoof_model\u001b[38;5;241m.\u001b[39mpredict(img, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(MODEL_DIR, model_name))\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(prediction) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: No valid anti-spoofing models were found or processed.\u001b[39m\u001b[38;5;124m\"\u001b[39m); \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[37], line 65\u001b[0m, in \u001b[0;36mCropImage.crop\u001b[1;34m(self, org_img, bbox, scale, out_w, out_h)\u001b[0m\n\u001b[0;32m     63\u001b[0m x1, y1, x2, y2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, [x1, y1, x1 \u001b[38;5;241m+\u001b[39m box_w, y1 \u001b[38;5;241m+\u001b[39m box_h]); h, w, _ \u001b[38;5;241m=\u001b[39m org_img\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     64\u001b[0m x1, y1, x2, y2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, x1), \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, y1), \u001b[38;5;28mmin\u001b[39m(w, x2), \u001b[38;5;28mmin\u001b[39m(h, y2)\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43morg_img\u001b[49m\u001b[43m[\u001b[49m\u001b[43my1\u001b[49m\u001b[43m:\u001b[49m\u001b[43my2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m:\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_h\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# FINAL, WORKING SCRIPT (v8 - Added Crop Safety Check)\n",
    "# ===================================================================\n",
    "\n",
    "# --- Force Jupyter to reload external files ---\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import datetime \n",
    "import sys\n",
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch import nn\n",
    "\n",
    "# --- Manually add the library's path for its internal imports ---\n",
    "sys.path.append(os.path.join('SilentFaceAntiSpoofing'))\n",
    "\n",
    "# --- Import the REAL model architectures ---\n",
    "from src.model_lib.MiniFASNet import MiniFASNetV1, MiniFASNetV2, MiniFASNetV1SE, MiniFASNetV2SE\n",
    "\n",
    "# --- Define the rest of the library classes directly ---\n",
    "def parse_model_name(model_name):\n",
    "    info = model_name.split('_'); dim_part = [p for p in info if 'x' in p][0]; dim_index = info.index(dim_part)\n",
    "    scale = float(info[dim_index - 1]); h, w = dim_part.split('x'); model_type = info[dim_index + 1].split('.')[0]\n",
    "    return int(h), int(w), model_type, scale\n",
    "def get_kernel(height, width):\n",
    "    return ((height + 15) // 16, (width + 15) // 16)\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, img): return torch.from_numpy(img.transpose((2, 0, 1))).float()\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms): self.transforms = transforms\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms: img = t(img)\n",
    "        return img\n",
    "\n",
    "MODEL_MAPPING = {'MiniFASNetV1': MiniFASNetV1, 'MiniFASNetV2': MiniFASNetV2, 'MiniFASNetV1SE':MiniFASNetV1SE, 'MiniFASNetV2SE':MiniFASNetV2SE}\n",
    "\n",
    "# --- THE FINAL FIX IS HERE ---\n",
    "class CropImage:\n",
    "    def crop(self, org_img, bbox, scale, out_w, out_h):\n",
    "        face_w, face_h = bbox[2], bbox[3]; x_center, y_center = bbox[0] + face_w / 2, bbox[1] + face_h / 2\n",
    "        box_w, box_h = face_w * scale, face_h * scale; x1, y1 = x_center - box_w / 2, y_center - box_h / 2\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x1 + box_w, y1 + box_h]); h, w, _ = org_img.shape\n",
    "        x1, y1, x2, y2 = max(0, x1), max(0, y1), min(w, x2), min(h, y2)\n",
    "        \n",
    "        cropped = org_img[y1:y2, x1:x2]\n",
    "        \n",
    "        # Safety check: if the crop is empty, return a black image of the target size.\n",
    "        if cropped.size == 0:\n",
    "            return np.zeros((out_h, out_w, 3), dtype=np.uint8)\n",
    "            \n",
    "        return cv2.resize(cropped, (out_w, out_h))\n",
    "# -----------------------------\n",
    "\n",
    "class Detection:\n",
    "    def __init__(self):\n",
    "        caffemodel = os.path.join('SilentFaceAntiSpoofing','resources','detection_model','Widerface-RetinaFace.caffemodel')\n",
    "        deploy = os.path.join('SilentFaceAntiSpoofing','resources','detection_model','deploy.prototxt')\n",
    "        self.detector = cv2.dnn.readNetFromCaffe(deploy, caffemodel)\n",
    "        self.detector_confidence = 0.6 \n",
    "    def get_bbox(self, img):\n",
    "        height, width, _ = img.shape; aspect_ratio = width / height\n",
    "        if img.shape[1] * img.shape[0] >= 192*192: img_resized = cv2.resize(img, (int(192*math.sqrt(aspect_ratio)), int(192/math.sqrt(aspect_ratio))), interpolation=cv2.INTER_LINEAR)\n",
    "        else: img_resized = img\n",
    "        blob = cv2.dnn.blobFromImage(img_resized, 1, mean=(104, 117, 123)); self.detector.setInput(blob, 'data')\n",
    "        out = self.detector.forward('detection_out').squeeze()\n",
    "        if out.ndim == 1 or len(out) == 0: return None\n",
    "        max_conf_index = np.argmax(out[:, 2])\n",
    "        if out[max_conf_index, 2] < self.detector_confidence: return None\n",
    "        left,top,right,bottom = out[max_conf_index,3]*width, out[max_conf_index,4]*height, out[max_conf_index,5]*width, out[max_conf_index,6]*height\n",
    "        return [int(left), int(top), int(right-left+1), int(bottom-top+1)]\n",
    "\n",
    "class AntiSpoofPredict(Detection):\n",
    "    def __init__(self, device_id):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.models = {}\n",
    "    def _load_model(self, model_path):\n",
    "        if model_path in self.models: self.model = self.models[model_path]; return\n",
    "        model_name = os.path.basename(model_path); h, w, model_type, scale = parse_model_name(model_name)\n",
    "        kernel_size = get_kernel(h, w)\n",
    "        self.model = MODEL_MAPPING[model_type](conv6_kernel=kernel_size).to(self.device)\n",
    "        state_dict = torch.load(model_path, map_location=self.device)\n",
    "        if next(iter(state_dict)).startswith('module.'):\n",
    "            from collections import OrderedDict\n",
    "            new_state_dict = OrderedDict((k[7:], v) for k, v in state_dict.items())\n",
    "            self.model.load_state_dict(new_state_dict)\n",
    "        else: self.model.load_state_dict(state_dict)\n",
    "        self.models[model_path] = self.model\n",
    "    def predict(self, img, model_path):\n",
    "        test_transform = Compose([ToTensor()]); img = test_transform(img).unsqueeze(0).to(self.device)\n",
    "        self._load_model(model_path); self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            result = self.model.forward(img); result = F.softmax(result, dim=1).cpu().numpy()\n",
    "        return result\n",
    "\n",
    "# ===================================================================\n",
    "# YOUR MAIN SCRIPT\n",
    "# ===================================================================\n",
    "\n",
    "ENCODINGS_PATH = 'encodings.pickle'\n",
    "LOG_FILE = 'log.txt'\n",
    "MODEL_DIR = os.path.join('SilentFaceAntiSpoofing', 'resources', 'anti_spoof_models')\n",
    "DEVICE_ID = 0\n",
    "\n",
    "print(\"Initializing models...\")\n",
    "anti_spoof_model = AntiSpoofPredict(DEVICE_ID)\n",
    "image_cropper = CropImage()\n",
    "print(\"Models initialized.\")\n",
    "\n",
    "def save_encoding(encodings, save_path):\n",
    "    with open(save_path, 'wb') as f: pickle.dump(encodings, f)\n",
    "def load_encoding(save_path):\n",
    "    with open(save_path, 'rb') as f: return pickle.load(f)\n",
    "\n",
    "encodeDict = {}\n",
    "if os.path.exists(ENCODINGS_PATH):\n",
    "    print(\"Loading known encodings...\"); encodeDict = load_encoding(ENCODINGS_PATH)\n",
    "    print(f\"Loaded {len(encodeDict)} known encodings.\")\n",
    "else: print(\"No existing encodings file found. A new one will be created.\")\n",
    "\n",
    "def is_real_face(image_frame):\n",
    "    image_bbox = anti_spoof_model.get_bbox(image_frame)\n",
    "    if image_bbox is None: print(\"Warning: No face detected for anti-spoofing check.\"); return 0 \n",
    "    prediction = np.zeros((1, 3))\n",
    "    model_filenames = [f for f in os.listdir(MODEL_DIR) if f.endswith(('.pth', '.onnx'))]\n",
    "    for model_name in model_filenames:\n",
    "        try: h, w, model_type, scale = parse_model_name(model_name)\n",
    "        except Exception as e: print(f\"Warning: Could not parse model name '{model_name}'. Skipping. Error: {e}\"); continue\n",
    "        param = { \"org_img\": image_frame, \"bbox\": image_bbox, \"scale\": scale, \"out_w\": w, \"out_h\": h }\n",
    "        img = image_cropper.crop(**param)\n",
    "        prediction += anti_spoof_model.predict(img, os.path.join(MODEL_DIR, model_name))\n",
    "    if np.sum(prediction) == 0: print(\"Error: No valid anti-spoofing models were found or processed.\"); return 0\n",
    "    label = np.argmax(prediction)\n",
    "    return label\n",
    "\n",
    "def recognize_face(frame):\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    boxes = face_recognition.face_locations(rgb, model='hog')\n",
    "    encs = face_recognition.face_encodings(rgb, boxes)\n",
    "    if not encs: return \"no_persons_found\"\n",
    "    face_enc = encs[0]\n",
    "    if not encodeDict: return \"unknown_person\"\n",
    "    known_encodings = np.array(list(encodeDict.values()))\n",
    "    distances = face_recognition.face_distance(known_encodings, face_enc)\n",
    "    best_idx = np.argmin(distances)\n",
    "    if distances[best_idx] < 0.6: return list(encodeDict.keys())[best_idx]\n",
    "    return \"unknown_person\"\n",
    "\n",
    "class MockArgs:\n",
    "    def __init__(self, image_path, name=None): self.image = image_path; self.name = name\n",
    "\n",
    "def do_action(action, args):\n",
    "    frame = cv2.imread(args.image)\n",
    "    if frame is None: print(f\"Error: Could not read image '{args.image}'.\"); return\n",
    "    live_label = is_real_face(frame)\n",
    "    if live_label != 1: print(\"âš  Spoofing attempt detected! Authentication failed.\"); return\n",
    "    print(\"âœ… Liveness check passed.\")\n",
    "    name = recognize_face(frame)\n",
    "    if action == 'register':\n",
    "        if name not in ['unknown_person', 'no_persons_found']: print(f\"User '{name}' is already registered.\"); return\n",
    "        if not args.name: print(\"--name is required for register action.\"); return\n",
    "        print(f\"Registering new user: {args.name}\")\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        encs = face_recognition.face_encodings(rgb_frame)\n",
    "        if not encs: print(\"Could not find a face to encode in the image.\"); return\n",
    "        encodeDict[args.name] = encs[0]\n",
    "        save_encoding(encodeDict, ENCODINGS_PATH) \n",
    "        print(f\"âœ… Registered and saved new user: {args.name}\")\n",
    "        return\n",
    "    if name in ['unknown_person', 'no_persons_found']: print(\"Unknown user. Please register first.\"); return\n",
    "    timestamp = datetime.datetime.now().isoformat(); mode = 'in' if action == 'login' else 'out'\n",
    "    with open(LOG_FILE, 'a') as f: f.write(f\"{name},{timestamp},{mode}\\n\")\n",
    "    if action == 'login': print(f\"âœ… Welcome, {name}!\")\n",
    "    else: print(f\"ðŸ‘‹ Goodbye, {name}!\")\n",
    "\n",
    "# --- EXAMPLE USAGE ---\n",
    "# IMPORTANT: If you had the 'inhomogeneous shape' error before,\n",
    "# DELETE the 'encodings.pickle' file before running this.\n",
    "print(\"--- REGISTERING A NEW USER ---\")\n",
    "register_args = MockArgs(image_path='my_face.jpg', name='John Doe')\n",
    "do_action('register', register_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dca06b-293b-4e99-9490-bdb298dbfc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
