{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f85c163-8fc1-4504-9379-901c2794f7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (4.11.0.86)\n",
      "Requirement already satisfied: face_recognition in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: torch in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (2.7.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (0.22.1)\n",
      "Requirement already satisfied: face-recognition-models>=0.3.0 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from face_recognition->-r requirements.txt (line 2)) (0.3.0)\n",
      "Requirement already satisfied: Click>=6.0 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from face_recognition->-r requirements.txt (line 2)) (8.1.7)\n",
      "Requirement already satisfied: dlib>=19.7 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from face_recognition->-r requirements.txt (line 2)) (19.24.99)\n",
      "Requirement already satisfied: Pillow in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from face_recognition->-r requirements.txt (line 2)) (10.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 4)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 4)) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 4)) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 4)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 4)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 4)) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 4)) (69.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kiit0001\\appdata\\roaming\\python\\python312\\site-packages (from Click>=6.0->face_recognition->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kiit0001\\anaconda3\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 4)) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r \"requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5fe38755-1bef-4a7d-95ca-54d2a36abc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading existing encodings...\n",
      "[INFO] Processing image: my_face.jpg\n",
      "[INFO] Added 'Ayushi Kumari' to the database.\n",
      "[INFO] Saving updated encodings...\n",
      "[INFO] Enrollment complete.\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "ENCODINGS_PATH = 'encodings.pickle'\n",
    "YOUR_NAME = \"Ayushi Kumari\" \n",
    "YOUR_PHOTO = \"my_face.jpg\"\n",
    "\n",
    "if os.path.exists(ENCODINGS_PATH):\n",
    "    print(\"[INFO] Loading existing encodings...\")\n",
    "    with open(ENCODINGS_PATH, \"rb\") as f:\n",
    "        encodeDict = pickle.load(f)\n",
    "else:\n",
    "    print(\"[INFO] No existing encodings file found. Creating a new one.\")\n",
    "    encodeDict = {}\n",
    "\n",
    "print(f\"[INFO] Processing image: {YOUR_PHOTO}\")\n",
    "image = cv2.imread(YOUR_PHOTO)\n",
    "rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "boxes = face_recognition.face_locations(rgb, model=\"hog\")\n",
    "encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "\n",
    "if encodings:\n",
    "\n",
    "    encodeDict[YOUR_NAME] = encodings[0]\n",
    "    print(f\"[INFO] Added '{YOUR_NAME}' to the database.\")\n",
    "else:\n",
    "    print(f\"[WARNING] No faces found in {YOUR_PHOTO}. Please use a clearer picture.\")\n",
    "\n",
    "print(\"[INFO] Saving updated encodings...\")\n",
    "with open(ENCODINGS_PATH, \"wb\") as f:\n",
    "    pickle.dump(encodeDict, f)\n",
    "\n",
    "print(\"[INFO] Enrollment complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bce760ee-f344-46f8-ae58-e9c3fff09681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import datetime \n",
    "import sys\n",
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch import nn\n",
    "\n",
    "sys.path.append(os.path.join('SilentFaceAntiSpoofing'))\n",
    "from src.model_lib.MiniFASNet import MiniFASNetV1, MiniFASNetV2, MiniFASNetV1SE, MiniFASNetV2SE\n",
    "\n",
    "def parse_model_name(model_name):\n",
    "    info = model_name.split('_'); dim_part = [p for p in info if 'x' in p][0]; dim_index = info.index(dim_part)\n",
    "    scale = float(info[dim_index - 1]); h, w = dim_part.split('x'); model_type = info[dim_index + 1].split('.')[0]\n",
    "    return int(h), int(w), model_type, scale\n",
    "def get_kernel(height, width):\n",
    "    return ((height + 15) // 16, (width + 15) // 16)\n",
    "class ToTensor(object):\n",
    "    def __call__(self, img): return torch.from_numpy(img.transpose((2, 0, 1))).float()\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms): self.transforms = transforms\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms: img = t(img)\n",
    "        return img\n",
    "MODEL_MAPPING = {'MiniFASNetV1': MiniFASNetV1, 'MiniFASNetV2': MiniFASNetV2, 'MiniFASNetV1SE':MiniFASNetV1SE, 'MiniFASNetV2SE':MiniFASNetV2SE}\n",
    "class CropImage:\n",
    "    def crop(self, org_img, bbox, scale, out_w, out_h):\n",
    "        face_w, face_h = bbox[2], bbox[3]; x_center, y_center = bbox[0] + face_w / 2, bbox[1] + face_h / 2\n",
    "        box_w, box_h = face_w * scale, face_h * scale; x1, y1 = x_center - box_w / 2, y_center - box_h / 2\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x1 + box_w, y1 + box_h]); h, w, _ = org_img.shape\n",
    "        x1, y1, x2, y2 = max(0, x1), max(0, y1), min(w, x2), min(h, y2)\n",
    "        cropped = org_img[y1:y2, x1:x2]\n",
    "        if cropped.size == 0: return np.zeros((out_h, out_w, 3), dtype=np.uint8)\n",
    "        return cv2.resize(cropped, (out_w, out_h))\n",
    "class Detection:\n",
    "    def __init__(self):\n",
    "        caffemodel = os.path.join('SilentFaceAntiSpoofing','resources','detection_model','Widerface-RetinaFace.caffemodel')\n",
    "        deploy = os.path.join('SilentFaceAntiSpoofing','resources','detection_model','deploy.prototxt')\n",
    "        self.detector = cv2.dnn.readNetFromCaffe(deploy, caffemodel)\n",
    "        self.detector_confidence = 0.6 \n",
    "    def get_bbox(self, img):\n",
    "        height, width, _ = img.shape; aspect_ratio = width / height\n",
    "        if img.shape[1] * img.shape[0] >= 192*192:\n",
    "            img_resized = cv2.resize(img, (int(192*math.sqrt(aspect_ratio)), int(192/math.sqrt(aspect_ratio))), interpolation=cv2.INTER_LINEAR)\n",
    "        else: img_resized = img\n",
    "        blob = cv2.dnn.blobFromImage(img_resized, 1, mean=(104, 117, 123)); self.detector.setInput(blob, 'data')\n",
    "        out = self.detector.forward('detection_out').squeeze()\n",
    "        if out.ndim == 1 or len(out) == 0: return None\n",
    "        max_conf_index = np.argmax(out[:, 2])\n",
    "        if out[max_conf_index, 2] < self.detector_confidence: return None\n",
    "        left,top,right,bottom = out[max_conf_index,3]*width, out[max_conf_index,4]*height, out[max_conf_index,5]*width, out[max_conf_index,6]*height\n",
    "        return [int(left), int(top), int(right-left+1), int(bottom-top+1)]\n",
    "class AntiSpoofPredict(Detection):\n",
    "    def __init__(self, device_id):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.models = {}\n",
    "    def _load_model(self, model_path):\n",
    "        if model_path in self.models: self.model = self.models[model_path]; return\n",
    "        model_name = os.path.basename(model_path); h, w, model_type, scale = parse_model_name(model_name)\n",
    "        kernel_size = get_kernel(h, w)\n",
    "        self.model = MODEL_MAPPING[model_type](conv6_kernel=kernel_size).to(self.device)\n",
    "        state_dict = torch.load(model_path, map_location=self.device)\n",
    "        if next(iter(state_dict)).startswith('module.'):\n",
    "            from collections import OrderedDict\n",
    "            new_state_dict = OrderedDict((k[7:], v) for k, v in state_dict.items())\n",
    "            self.model.load_state_dict(new_state_dict)\n",
    "        else: self.model.load_state_dict(state_dict)\n",
    "        self.models[model_path] = self.model\n",
    "    def predict(self, img, model_path):\n",
    "        test_transform = Compose([ToTensor()]); img = test_transform(img).unsqueeze(0).to(self.device)\n",
    "        self._load_model(model_path); self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            result = self.model.forward(img); result = F.softmax(result, dim=1).cpu().numpy()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fa7f6ebf-6855-4082-b364-9809cfc55445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n",
      "Models initialized.\n",
      "[INFO] Loading encodings...\n",
      "[INFO] Loaded 112 known user encodings.\n",
      "[INFO] Starting live camera feed. Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ENCODINGS_PATH = 'encodings.pickle'\n",
    "LOG_FILE = 'log.t'\n",
    "MODEL_DIR = os.path.join('SilentFaceAntiSpoofing', 'resources', 'anti_spoof_models')\n",
    "DEVICE_ID = 0\n",
    "\n",
    "print(\"Initializing models...\")\n",
    "anti_spoof_model = AntiSpoofPredict(DEVICE_ID)\n",
    "image_cropper = CropImage()\n",
    "print(\"Models initialized.\")\n",
    "\n",
    "if not os.path.exists(ENCODINGS_PATH):\n",
    "    print(f\"[ERROR] Encodings file not found at '{ENCODINGS_PATH}'.\")\n",
    "    encodeDict = None\n",
    "else:\n",
    "    print(\"[INFO] Loading encodings...\")\n",
    "    with open(ENCODINGS_PATH, \"rb\") as f:\n",
    "        encodeDict = pickle.load(f)\n",
    "    print(f\"[INFO] Loaded {len(encodeDict)} known user encodings.\")\n",
    "\n",
    "\n",
    "def recognize_face(rgb_frame, known_encodings_dict):\n",
    "    if known_encodings_dict is None:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    boxes = face_recognition.face_locations(rgb_frame, model='hog')\n",
    "    current_encodings = face_recognition.face_encodings(rgb_frame, boxes)\n",
    "    \n",
    "    if not current_encodings:\n",
    "        return \"Unknown\" \n",
    "    \n",
    "    face_enc_to_check = current_encodings[0]\n",
    "\n",
    "    for name, known_encoding in known_encodings_dict.items():\n",
    "        try:\n",
    "            match = face_recognition.compare_faces([known_encoding], face_enc_to_check)\n",
    "            if match[0]: \n",
    "                return name\n",
    "        except Exception as e:\n",
    "            continue\n",
    "            \n",
    "    return \"Unknown\"\n",
    "    \n",
    "def is_real_face(image_frame):\n",
    "    image_bbox = anti_spoof_model.get_bbox(image_frame)\n",
    "    if image_bbox is None: return 0, None\n",
    "    prediction = np.zeros((1, 3))\n",
    "    model_filenames = [f for f in os.listdir(MODEL_DIR) if f.endswith(('.pth', '.onnx'))]\n",
    "    for model_name in model_filenames:\n",
    "        try: h, w, model_type, scale = parse_model_name(model_name)\n",
    "        except Exception as e: print(f\"Warning: Could not parse model name '{model_name}'. Skipping.\"); continue\n",
    "        param = { \"org_img\": image_frame, \"bbox\": image_bbox, \"scale\": scale, \"out_w\": w, \"out_h\": h }\n",
    "        img = image_cropper.crop(**param)\n",
    "        prediction += anti_spoof_model.predict(img, os.path.join(MODEL_DIR, model_name))\n",
    "    if np.sum(prediction) == 0: return 0, image_bbox\n",
    "    label = np.argmax(prediction)\n",
    "    return label, image_bbox\n",
    "\n",
    "def start_live_check():\n",
    "    if encodeDict is None:\n",
    "        print(\"[ERROR] Cannot start live check, encodings not loaded.\")\n",
    "        return\n",
    "\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    if not video_capture.isOpened():\n",
    "        print(\"[ERROR] Could not open video stream.\")\n",
    "        return\n",
    "        \n",
    "    print(\"[INFO] Starting live camera feed. Press 'q' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        live_label, bbox = is_real_face(frame)\n",
    "        \n",
    "        name = \"Unknown\"\n",
    "        text_color = (0, 0, 255)\n",
    "\n",
    "        if live_label == 1:\n",
    "\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            name = recognize_face(rgb_frame, encodeDict)\n",
    "            if name != \"Unknown\":\n",
    "                text_color = (0, 255, 0)\n",
    "        \n",
    "        status = \"Spoof\" if live_label != 1 else name\n",
    "\n",
    "        if bbox is not None:\n",
    "            x, y, w, h = bbox\n",
    "            box_color = (0, 255, 0) if live_label == 1 else (0, 0, 255)\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), box_color, 2)\n",
    "            y_text = y - 10 if y - 10 > 10 else y + h + 25\n",
    "            cv2.putText(frame, status, (x, y_text), cv2.FONT_HERSHEY_SIMPLEX, 0.75, text_color, 2)\n",
    "        else:\n",
    "            cv2.putText(frame, \"No face detected\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 255, 255), 2)\n",
    "\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "start_live_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bd03e8-d0a9-4ffb-8a92-2f85c02f7ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
